{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659a10fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/sakshamjn/vehicle-detection-8-classes-object-detection\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Install kaggle package\n",
    "subprocess.run(['pip', 'install', 'kaggle'])\n",
    "\n",
    "# Import necessary libraries\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "# Authenticate with your Kaggle credentials\n",
    "api = KaggleApi()\n",
    "api.authenticate()  # Make sure to authenticate with your Kaggle credentials\n",
    "\n",
    "# Download the Road Vehicle Images Dataset\n",
    "api.dataset_download_files('sakshamjn/vehicle-detection-8-classes-object-detection', path = 'kaggle-dataset/'\n",
    ", unzip=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10727814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/sakshamjn/vehicle-detection-8-classes-object-detection\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "# Install kaggle package\n",
    "subprocess.run(['pip', 'install', 'kaggle'])\n",
    "\n",
    "# Import necessary libraries\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "# Authenticate with your Kaggle credentials\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "# Define the download path\n",
    "download_path = 'C:/Users/mahnoor/Documents/Python Scripts/kaggle-dataset/'\n",
    "\n",
    "# Download the Road Vehicle Images Dataset\n",
    "api.dataset_download_files('sakshamjn/vehicle-detection-8-classes-object-detection', path=download_path, unzip=True)\n",
    "\n",
    "# Unzip the dataset if not already unzipped\n",
    "zip_file_path = os.path.join(download_path, 'vehicle-detection-8-classes-object-detection.zip')\n",
    "\n",
    "if os.path.exists(zip_file_path):\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(download_path)\n",
    "    os.remove(zip_file_path)  # Remove the zip file after extracting\n",
    "\n",
    "print(\"Dataset downloaded and extracted successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0351b318",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2174388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels shape: (8218,)\n",
      "First few labels: [array([[3.      , 0.509804, 0.411765, 0.107843, 0.245098],\n",
      "        [2.      , 0.210784, 0.616422, 0.127451, 0.232843]], dtype=float32)\n",
      " array([[0.     , 0.50375, 0.405  , 0.0975 , 0.245  ],\n",
      "        [1.     , 0.19   , 0.59875, 0.12   , 0.2325 ],\n",
      "        [2.     , 0.62125, 0.61   , 0.0575 , 0.125  ]], dtype=float32)\n",
      " array([[3.      , 0.516169, 0.396766, 0.10199 , 0.246269],\n",
      "        [2.      , 0.205224, 0.594527, 0.121891, 0.233831],\n",
      "        [4.      , 0.631841, 0.597015, 0.059701, 0.124378]], dtype=float32)\n",
      " array([[2.      , 0.141827, 0.539663, 0.110577, 0.165865],\n",
      "        [2.      , 0.360577, 0.278846, 0.0625  , 0.086538],\n",
      "        [2.      , 0.634615, 0.649038, 0.0625  , 0.129808]], dtype=float32)\n",
      " array([[2.      , 0.140625, 0.528846, 0.117788, 0.168269],\n",
      "        [2.      , 0.366587, 0.274038, 0.064904, 0.086538],\n",
      "        [2.      , 0.628606, 0.653846, 0.064904, 0.129808]], dtype=float32)]\n",
      "Warning: Skipping image C:/Users/mahnoor/Documents/Python Scripts/kaggle-dataset/train/images\\ulu1083_jpg.rf.979d73c7e4c14e145aa82c5d9f38c14c.jpg due to missing or invalid label\n",
      "Warning: Skipping image C:/Users/mahnoor/Documents/Python Scripts/kaggle-dataset/train/images\\ulu1141_jpg.rf.e8f1a19531d0b8fea9703f53a429ff4c.jpg due to missing or invalid label\n",
      "Warning: Skipping image C:/Users/mahnoor/Documents/Python Scripts/kaggle-dataset/train/images\\ulu1148_jpg.rf.c3b8bd372d7d0a4eb580f7e9ae6a4422.jpg due to missing or invalid label\n",
      "Warning: Skipping image C:/Users/mahnoor/Documents/Python Scripts/kaggle-dataset/train/images\\ulu1164_jpg.rf.6c7f61bdcbe103de8f9c503e5d6ef866.jpg due to missing or invalid label\n",
      "Warning: Skipping image C:/Users/mahnoor/Documents/Python Scripts/kaggle-dataset/train/images\\ulu1231_jpg.rf.b72e53b414dec92e1b8e75c404523d56.jpg due to missing or invalid label\n",
      "Warning: Skipping image C:/Users/mahnoor/Documents/Python Scripts/kaggle-dataset/train/images\\ulu1236_jpg.rf.966ffc233912af8701c8da397b1f72f7.jpg due to missing or invalid label\n",
      "Warning: Skipping image C:/Users/mahnoor/Documents/Python Scripts/kaggle-dataset/train/images\\ulu1307_jpg.rf.9c8621d9a0f4acd7353f235c69919416.jpg due to missing or invalid label\n",
      "Warning: Skipping image C:/Users/mahnoor/Documents/Python Scripts/kaggle-dataset/train/images\\ulu1337_jpg.rf.1c0bf2fa9b1a8a13b313affdc48c7894.jpg due to missing or invalid label\n",
      "Warning: Skipping image C:/Users/mahnoor/Documents/Python Scripts/kaggle-dataset/train/images\\ulu1395_jpg.rf.c8837e6eb73491ce738a98c261a8c332.jpg due to missing or invalid label\n",
      "Warning: Skipping image C:/Users/mahnoor/Documents/Python Scripts/kaggle-dataset/train/images\\ulu1398_jpg.rf.e01c3b8499149cefe52af0f4ecc32646.jpg due to missing or invalid label\n",
      "Warning: Skipping image C:/Users/mahnoor/Documents/Python Scripts/kaggle-dataset/train/images\\ulu1537_jpg.rf.499a838a4152eb610d7328efa96311c3.jpg due to missing or invalid label\n",
      "Warning: Skipping image C:/Users/mahnoor/Documents/Python Scripts/kaggle-dataset/train/images\\ulu1574_jpg.rf.da29e82c89670e9cce89bac613fc4b71.jpg due to missing or invalid label\n",
      "Warning: Skipping image C:/Users/mahnoor/Documents/Python Scripts/kaggle-dataset/train/images\\ulu1587_jpg.rf.1d3059744070e1621334f3bef963c024.jpg due to missing or invalid label\n",
      "Warning: Skipping image C:/Users/mahnoor/Documents/Python Scripts/kaggle-dataset/train/images\\ulu1663_jpg.rf.b0054603c1adda73fdce62e78cc9db04.jpg due to missing or invalid label\n",
      "Warning: Skipping image C:/Users/mahnoor/Documents/Python Scripts/kaggle-dataset/train/images\\ulu1699_jpg.rf.87acfcaf25d9159ada584b6a14659152.jpg due to missing or invalid label\n",
      "Warning: Skipping image C:/Users/mahnoor/Documents/Python Scripts/kaggle-dataset/train/images\\ulu2805_jpg.rf.db33c0d0568f9b0aa7419f75cbef4adc.jpg due to missing or invalid label\n",
      "Warning: Skipping image C:/Users/mahnoor/Documents/Python Scripts/kaggle-dataset/train/images\\ulu652_jpg.rf.eeab3e8c14e526a468aac81a115828ae.jpg due to missing or invalid label\n",
      "Warning: Skipping image C:/Users/mahnoor/Documents/Python Scripts/kaggle-dataset/train/images\\ulu897_jpg.rf.b86203744f6e3c6d5a55af69093edd85.jpg due to missing or invalid label\n",
      "WARNING:tensorflow:From C:\\Users\\mahnoor\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\mahnoor\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\mahnoor\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From C:\\Users\\mahnoor\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\mahnoor\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "205/205 [==============================] - 82s 376ms/step - loss: 1.5343 - accuracy: 0.5160 - val_loss: 1.5229 - val_accuracy: 0.5146\n",
      "Epoch 2/10\n",
      "205/205 [==============================] - 75s 365ms/step - loss: 1.4538 - accuracy: 0.5221 - val_loss: 1.5098 - val_accuracy: 0.5098\n",
      "Epoch 3/10\n",
      "205/205 [==============================] - 78s 378ms/step - loss: 1.3485 - accuracy: 0.5392 - val_loss: 1.4601 - val_accuracy: 0.5140\n",
      "Epoch 4/10\n",
      "205/205 [==============================] - 71s 348ms/step - loss: 1.1956 - accuracy: 0.5797 - val_loss: 1.4723 - val_accuracy: 0.5220\n",
      "Epoch 5/10\n",
      "205/205 [==============================] - 73s 357ms/step - loss: 0.9929 - accuracy: 0.6494 - val_loss: 1.5670 - val_accuracy: 0.5140\n",
      "Epoch 6/10\n",
      "205/205 [==============================] - 77s 378ms/step - loss: 0.7560 - accuracy: 0.7364 - val_loss: 1.6507 - val_accuracy: 0.5171\n",
      "Epoch 7/10\n",
      "205/205 [==============================] - 72s 352ms/step - loss: 0.5185 - accuracy: 0.8212 - val_loss: 2.0248 - val_accuracy: 0.5049\n",
      "Epoch 8/10\n",
      "205/205 [==============================] - 70s 341ms/step - loss: 0.3210 - accuracy: 0.8939 - val_loss: 2.3058 - val_accuracy: 0.5073\n",
      "Epoch 9/10\n",
      "205/205 [==============================] - 77s 374ms/step - loss: 0.2008 - accuracy: 0.9401 - val_loss: 2.4970 - val_accuracy: 0.5244\n",
      "Epoch 10/10\n",
      "205/205 [==============================] - 83s 404ms/step - loss: 0.1227 - accuracy: 0.9668 - val_loss: 2.6974 - val_accuracy: 0.5280\n",
      "52/52 [==============================] - 4s 82ms/step - loss: 2.6974 - accuracy: 0.5280\n",
      "Validation Accuracy: 52.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mahnoor\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained and saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define dataset paths\n",
    "images_path = 'C:/Users/mahnoor/Documents/Python Scripts/kaggle-dataset/train/images'\n",
    "labels_path = 'C:/Users/mahnoor/Documents/Python Scripts/kaggle-dataset/train/labels.npy'\n",
    "\n",
    "# Check if the dataset paths exist\n",
    "if not os.path.exists(images_path):\n",
    "    raise FileNotFoundError(f\"The specified images path does not exist: {images_path}\")\n",
    "\n",
    "if not os.path.exists(labels_path):\n",
    "    raise FileNotFoundError(f\"The specified labels path does not exist: {labels_path}\")\n",
    "\n",
    "# Load labels\n",
    "labels = np.load(labels_path, allow_pickle=True)\n",
    "\n",
    "# Print the shape and a few entries of the labels array to understand its structure\n",
    "print(f\"Labels shape: {labels.shape}\")\n",
    "print(f\"First few labels: {labels[:5]}\")\n",
    "\n",
    "# Extract only the class labels from the complex label structures\n",
    "class_labels = []\n",
    "for label in labels:\n",
    "    if len(label) > 0:\n",
    "        class_labels.append(label[0][0])\n",
    "    else:\n",
    "        class_labels.append(-1)  # Placeholder for missing labels\n",
    "\n",
    "class_labels = np.array(class_labels)\n",
    "\n",
    "# Load images and corresponding labels\n",
    "images = []\n",
    "filtered_labels = []\n",
    "image_files = sorted([f for f in os.listdir(images_path) if f.endswith(('png', 'jpg', 'jpeg'))])\n",
    "\n",
    "for idx, img_name in enumerate(image_files):\n",
    "    img_path = os.path.join(images_path, img_name)\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is not None and class_labels[idx] != -1:\n",
    "        img = cv2.resize(img, (128, 128))  # Resize images to 128x128\n",
    "        images.append(img)\n",
    "        filtered_labels.append(class_labels[idx])\n",
    "    else:\n",
    "        print(f\"Warning: Skipping image {img_path} due to missing or invalid label\")\n",
    "\n",
    "images = np.array(images)\n",
    "filtered_labels = np.array(filtered_labels)\n",
    "\n",
    "# Ensure the number of images matches the number of filtered labels\n",
    "if len(images) != len(filtered_labels):\n",
    "    raise ValueError(f\"Number of images ({len(images)}) does not match number of filtered labels ({len(filtered_labels)})\")\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(images, filtered_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the images\n",
    "X_train = X_train / 255.0\n",
    "X_val = X_val / 255.0\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "num_classes = len(np.unique(filtered_labels))\n",
    "y_train = to_categorical(y_train, num_classes=num_classes)\n",
    "y_val = to_categorical(y_val, num_classes=num_classes)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_val, y_val)\n",
    "print(f'Validation Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Save the model\n",
    "model.save('vehicle_detection_model.h5')\n",
    "\n",
    "print(\"Model trained and saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc1c62a",
   "metadata": {},
   "source": [
    "# Transfer Learning (Training on pre-trained model)      ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db174c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels shape: (8218,)\n",
      "First few labels: [array([[3.      , 0.509804, 0.411765, 0.107843, 0.245098],\n",
      "        [2.      , 0.210784, 0.616422, 0.127451, 0.232843]], dtype=float32)\n",
      " array([[0.     , 0.50375, 0.405  , 0.0975 , 0.245  ],\n",
      "        [1.     , 0.19   , 0.59875, 0.12   , 0.2325 ],\n",
      "        [2.     , 0.62125, 0.61   , 0.0575 , 0.125  ]], dtype=float32)\n",
      " array([[3.      , 0.516169, 0.396766, 0.10199 , 0.246269],\n",
      "        [2.      , 0.205224, 0.594527, 0.121891, 0.233831],\n",
      "        [4.      , 0.631841, 0.597015, 0.059701, 0.124378]], dtype=float32)\n",
      " array([[2.      , 0.141827, 0.539663, 0.110577, 0.165865],\n",
      "        [2.      , 0.360577, 0.278846, 0.0625  , 0.086538],\n",
      "        [2.      , 0.634615, 0.649038, 0.0625  , 0.129808]], dtype=float32)\n",
      " array([[2.      , 0.140625, 0.528846, 0.117788, 0.168269],\n",
      "        [2.      , 0.366587, 0.274038, 0.064904, 0.086538],\n",
      "        [2.      , 0.628606, 0.653846, 0.064904, 0.129808]], dtype=float32)]\n",
      "Warning: Skipping image C:/Users/mahnoor/Documents/Python Scripts/kaggle-dataset/train/images\\ulu1083_jpg.rf.979d73c7e4c14e145aa82c5d9f38c14c.jpg due to missing or invalid label\n",
      "Warning: Skipping image C:/Users/mahnoor/Documents/Python Scripts/kaggle-dataset/train/images\\ulu1141_jpg.rf.e8f1a19531d0b8fea9703f53a429ff4c.jpg due to missing or invalid label\n",
      "Warning: Skipping image C:/Users/mahnoor/Documents/Python Scripts/kaggle-dataset/train/images\\ulu1148_jpg.rf.c3b8bd372d7d0a4eb580f7e9ae6a4422.jpg due to missing or invalid label\n",
      "Warning: Skipping image C:/Users/mahnoor/Documents/Python Scripts/kaggle-dataset/train/images\\ulu1164_jpg.rf.6c7f61bdcbe103de8f9c503e5d6ef866.jpg due to missing or invalid label\n",
      "Warning: Skipping image C:/Users/mahnoor/Documents/Python Scripts/kaggle-dataset/train/images\\ulu1231_jpg.rf.b72e53b414dec92e1b8e75c404523d56.jpg due to missing or invalid label\n",
      "Warning: Skipping image C:/Users/mahnoor/Documents/Python Scripts/kaggle-dataset/train/images\\ulu1236_jpg.rf.966ffc233912af8701c8da397b1f72f7.jpg due to missing or invalid label\n",
      "Warning: Skipping image C:/Users/mahnoor/Documents/Python Scripts/kaggle-dataset/train/images\\ulu1307_jpg.rf.9c8621d9a0f4acd7353f235c69919416.jpg due to missing or invalid label\n",
      "Warning: Skipping image C:/Users/mahnoor/Documents/Python Scripts/kaggle-dataset/train/images\\ulu1337_jpg.rf.1c0bf2fa9b1a8a13b313affdc48c7894.jpg due to missing or invalid label\n",
      "Warning: Skipping image C:/Users/mahnoor/Documents/Python Scripts/kaggle-dataset/train/images\\ulu1395_jpg.rf.c8837e6eb73491ce738a98c261a8c332.jpg due to missing or invalid label\n",
      "Warning: Skipping image C:/Users/mahnoor/Documents/Python Scripts/kaggle-dataset/train/images\\ulu1398_jpg.rf.e01c3b8499149cefe52af0f4ecc32646.jpg due to missing or invalid label\n",
      "Warning: Skipping image C:/Users/mahnoor/Documents/Python Scripts/kaggle-dataset/train/images\\ulu1537_jpg.rf.499a838a4152eb610d7328efa96311c3.jpg due to missing or invalid label\n",
      "Warning: Skipping image C:/Users/mahnoor/Documents/Python Scripts/kaggle-dataset/train/images\\ulu1574_jpg.rf.da29e82c89670e9cce89bac613fc4b71.jpg due to missing or invalid label\n",
      "Warning: Skipping image C:/Users/mahnoor/Documents/Python Scripts/kaggle-dataset/train/images\\ulu1587_jpg.rf.1d3059744070e1621334f3bef963c024.jpg due to missing or invalid label\n",
      "Warning: Skipping image C:/Users/mahnoor/Documents/Python Scripts/kaggle-dataset/train/images\\ulu1663_jpg.rf.b0054603c1adda73fdce62e78cc9db04.jpg due to missing or invalid label\n",
      "Warning: Skipping image C:/Users/mahnoor/Documents/Python Scripts/kaggle-dataset/train/images\\ulu1699_jpg.rf.87acfcaf25d9159ada584b6a14659152.jpg due to missing or invalid label\n",
      "Warning: Skipping image C:/Users/mahnoor/Documents/Python Scripts/kaggle-dataset/train/images\\ulu2805_jpg.rf.db33c0d0568f9b0aa7419f75cbef4adc.jpg due to missing or invalid label\n",
      "Warning: Skipping image C:/Users/mahnoor/Documents/Python Scripts/kaggle-dataset/train/images\\ulu652_jpg.rf.eeab3e8c14e526a468aac81a115828ae.jpg due to missing or invalid label\n",
      "Warning: Skipping image C:/Users/mahnoor/Documents/Python Scripts/kaggle-dataset/train/images\\ulu897_jpg.rf.b86203744f6e3c6d5a55af69093edd85.jpg due to missing or invalid label\n",
      "Filtered images shape: (8200, 128, 128, 3)\n",
      "Filtered labels shape: (8200,)\n",
      "First few filtered labels: [3. 0. 3. 2. 2.]\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94765736/94765736 [==============================] - 40s 0us/step\n",
      "Epoch 1/50\n",
      "205/205 [==============================] - 312s 1s/step - loss: 2.3096 - accuracy: 0.1151 - val_loss: 2.0026 - val_accuracy: 0.3530 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "205/205 [==============================] - 250s 1s/step - loss: 2.1721 - accuracy: 0.1082 - val_loss: 2.1049 - val_accuracy: 0.0799 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "205/205 [==============================] - 268s 1s/step - loss: 2.1120 - accuracy: 0.1006 - val_loss: 2.0767 - val_accuracy: 0.0457 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "205/205 [==============================] - 327s 2s/step - loss: 2.1061 - accuracy: 0.0872 - val_loss: 2.0775 - val_accuracy: 0.0646 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "205/205 [==============================] - 285s 1s/step - loss: 2.0837 - accuracy: 0.0755 - val_loss: 2.0501 - val_accuracy: 0.0659 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "205/205 [==============================] - 279s 1s/step - loss: 2.0752 - accuracy: 0.0776 - val_loss: 2.0504 - val_accuracy: 0.1774 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "205/205 [==============================] - 258s 1s/step - loss: 2.0706 - accuracy: 0.0720 - val_loss: 2.0524 - val_accuracy: 0.0780 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "205/205 [==============================] - 252s 1s/step - loss: 2.0729 - accuracy: 0.0750 - val_loss: 2.0734 - val_accuracy: 0.0293 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "205/205 [==============================] - 250s 1s/step - loss: 2.0648 - accuracy: 0.0721 - val_loss: 2.0761 - val_accuracy: 0.0140 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "205/205 [==============================] - 250s 1s/step - loss: 2.0646 - accuracy: 0.0812 - val_loss: 2.0388 - val_accuracy: 0.2902 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "205/205 [==============================] - 255s 1s/step - loss: 2.0645 - accuracy: 0.0806 - val_loss: 2.0628 - val_accuracy: 0.0713 - lr: 0.0010\n",
      "52/52 [==============================] - 55s 1s/step - loss: 2.0026 - accuracy: 0.3530\n",
      "Validation Accuracy: 35.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mahnoor\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained and saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, GlobalAveragePooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import compute_class_weight\n",
    "\n",
    "# Define dataset paths\n",
    "images_path = 'C:/Users/mahnoor/Documents/Python Scripts/coco2017/train2017/images'\n",
    "labels_path = 'C:/Users/mahnoor/Documents/Python Scripts/coco2017/train/labels.npy'\n",
    "\n",
    "# Check if the dataset paths exist\n",
    "if not os.path.exists(images_path):\n",
    "    raise FileNotFoundError(f\"The specified images path does not exist: {images_path}\")\n",
    "\n",
    "if not os.path.exists(labels_path):\n",
    "    raise FileNotFoundError(f\"The specified labels path does not exist: {labels_path}\")\n",
    "\n",
    "# Load labels\n",
    "labels = np.load(labels_path, allow_pickle=True)\n",
    "\n",
    "# Print the shape and a few entries of the labels array to understand its structure\n",
    "print(f\"Labels shape: {labels.shape}\")\n",
    "print(f\"First few labels: {labels[:5]}\")\n",
    "\n",
    "# Extract only the class labels from the complex label structures\n",
    "class_labels = []\n",
    "for label in labels:\n",
    "    if len(label) > 0:\n",
    "        class_labels.append(label[0][0])\n",
    "    else:\n",
    "        class_labels.append(-1)  # Placeholder for missing labels\n",
    "\n",
    "class_labels = np.array(class_labels)\n",
    "\n",
    "# Load images and corresponding labels\n",
    "images = []\n",
    "filtered_labels = []\n",
    "image_files = sorted([f for f in os.listdir(images_path) if f.endswith(('png', 'jpg', 'jpeg'))])\n",
    "\n",
    "for idx, img_name in enumerate(image_files):\n",
    "    img_path = os.path.join(images_path, img_name)\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is not None and class_labels[idx] != -1:\n",
    "        img = cv2.resize(img, (128, 128))  # Resize images to 128x128\n",
    "        images.append(img)\n",
    "        filtered_labels.append(class_labels[idx])\n",
    "    else:\n",
    "        print(f\"Warning: Skipping image {img_path} due to missing or invalid label\")\n",
    "\n",
    "images = np.array(images)\n",
    "filtered_labels = np.array(filtered_labels)\n",
    "\n",
    "# Print the shape and a few entries of the filtered datasets\n",
    "print(f\"Filtered images shape: {images.shape}\")\n",
    "print(f\"Filtered labels shape: {filtered_labels.shape}\")\n",
    "print(f\"First few filtered labels: {filtered_labels[:5]}\")\n",
    "\n",
    "# Ensure the number of images matches the number of filtered labels\n",
    "if len(images) != len(filtered_labels):\n",
    "    raise ValueError(f\"Number of images ({len(images)}) does not match number of filtered labels ({len(filtered_labels)})\")\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(images, filtered_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the images\n",
    "X_train = X_train / 255.0\n",
    "X_val = X_val / 255.0\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "num_classes = len(np.unique(filtered_labels))\n",
    "y_train = to_categorical(y_train, num_classes=num_classes)\n",
    "y_val = to_categorical(y_val, num_classes=num_classes)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(filtered_labels), y=filtered_labels)\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "# Data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.3,\n",
    "    height_shift_range=0.3,\n",
    "    shear_range=0.3,\n",
    "    zoom_range=0.3,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Load the pre-trained ResNet50 model\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
    "\n",
    "# Freeze the base model\n",
    "base_model.trainable = False\n",
    "\n",
    "# Add custom layers on top of the base model\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = BatchNormalization()(x)\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "# Create the complete model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define callbacks\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(datagen.flow(X_train, y_train, batch_size=32),\n",
    "                    epochs=50, validation_data=(X_val, y_val),\n",
    "                    class_weight=class_weights, callbacks=[reduce_lr, early_stopping])\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_val, y_val)\n",
    "print(f'Validation Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Save the model\n",
    "model.save('vehicle_detection_model_resnet50_improved.h5')\n",
    "\n",
    "print(\"Model trained and saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41600196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model('vehicle_detection_model.h5')\n",
    "\n",
    "# Initialize video capture (0 for default camera, or provide video file path)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Parameters\n",
    "input_size = (128, 128)  # Input size for the model\n",
    "stationary_threshold = 15  # Time in seconds (2 minutes)\n",
    "stationary_objects = {}  # Dictionary to track stationary objects\n",
    "\n",
    "def preprocess_frame(frame):\n",
    "    \"\"\"Preprocess frame for model prediction.\"\"\"\n",
    "    img = cv2.resize(frame, input_size)\n",
    "    img = img / 255.0  # Normalize\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    return img\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Get frame dimensions\n",
    "    height, width = frame.shape[:2]\n",
    "\n",
    "    # Preprocess the frame for prediction\n",
    "    img = preprocess_frame(frame)\n",
    "    \n",
    "    # Predict using the model\n",
    "    predictions = model.predict(img)\n",
    "    predicted_class = np.argmax(predictions, axis=1)[0]\n",
    "\n",
    "    # Only consider vehicle classes (assuming vehicle classes are labeled from 1 to N)\n",
    "    if predicted_class in [0, 1, 2, 3, 4, 5, 6, 7]:  # Adjust based on your class indices\n",
    "        # For simplicity, assume the entire frame is the bounding box\n",
    "        bbox = (0, 0, width, height)  # You would normally get this from your detection model\n",
    "        \n",
    "        # Track stationary objects\n",
    "        if predicted_class not in stationary_objects:\n",
    "            stationary_objects[predicted_class] = {'start_time': time.time(), 'bbox': bbox}\n",
    "        else:\n",
    "            elapsed_time = time.time() - stationary_objects[predicted_class]['start_time']\n",
    "            if elapsed_time > stationary_threshold:\n",
    "                # Draw red bounding box around the vehicle\n",
    "                (x, y, w, h) = stationary_objects[predicted_class]['bbox']\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "            else:\n",
    "                stationary_objects[predicted_class]['bbox'] = bbox\n",
    "    else:\n",
    "        # Reset the timer if the object moves\n",
    "        if predicted_class in stationary_objects:\n",
    "            del stationary_objects[predicted_class]\n",
    "\n",
    "    # Display the frame with bounding boxes\n",
    "    cv2.imshow('Static Object Detection', frame)\n",
    "\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d0251ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video created successfully.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Path to the static car image\n",
    "car_image_path = r\"C:\\Users\\mahnoor\\Documents\\Python Scripts\\static_car.jpg\"\n",
    "\n",
    "# Load the car image\n",
    "car_image = cv2.imread(car_image_path)\n",
    "\n",
    "# Define video parameters\n",
    "frame_width = car_image.shape[1]\n",
    "frame_height = car_image.shape[0]\n",
    "fps = 30  # Frames per second\n",
    "duration = 120  # Duration in seconds\n",
    "total_frames = fps * duration\n",
    "\n",
    "# Define the codec and create VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter('static_car_video.avi', fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "# Write the same frame multiple times to create a video\n",
    "for _ in range(total_frames):\n",
    "    out.write(car_image)\n",
    "\n",
    "# Release the video writer object\n",
    "out.release()\n",
    "\n",
    "print(\"Video created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04c2f4c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mahnoor\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\mahnoor\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\mahnoor\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "1/1 [==============================] - 0s 484ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model('vehicle_detection_model.h5')\n",
    "\n",
    "# Initialize video capture with the generated video file\n",
    "video_path = 'static_car_video.avi'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Parameters\n",
    "input_size = (128, 128)  # Input size for the model\n",
    "stationary_threshold = 120  # Time in seconds (2 minutes)\n",
    "stationary_objects = {}  # Dictionary to track stationary objects\n",
    "blink_flag = True  # Flag to toggle the blinking effect\n",
    "blink_interval = 10  # Interval for blinking effect (in frames)\n",
    "\n",
    "def preprocess_frame(frame):\n",
    "    \"\"\"Preprocess frame for model prediction.\"\"\"\n",
    "    img = cv2.resize(frame, input_size)\n",
    "    img = img / 255.0  # Normalize\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    return img\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Get frame dimensions\n",
    "    height, width = frame.shape[:2]\n",
    "\n",
    "    # Preprocess the frame for prediction\n",
    "    img = preprocess_frame(frame)\n",
    "    \n",
    "    # Predict using the model\n",
    "    predictions = model.predict(img)\n",
    "    predicted_class = np.argmax(predictions, axis=1)[0]\n",
    "\n",
    "    # Only consider vehicle classes (assuming vehicle classes are labeled from 1 to N)\n",
    "    if predicted_class in [0, 1, 2, 3, 4, 5, 6, 7]:  # Adjust based on your class indices\n",
    "        # For simplicity, assume the entire frame is the bounding box\n",
    "        bbox = (0, 0, width, height)  # You would normally get this from your detection model\n",
    "        \n",
    "        # Track stationary objects\n",
    "        if predicted_class not in stationary_objects:\n",
    "            stationary_objects[predicted_class] = {'start_time': time.time(), 'bbox': bbox}\n",
    "        else:\n",
    "            elapsed_time = time.time() - stationary_objects[predicted_class]['start_time']\n",
    "            if elapsed_time > stationary_threshold:\n",
    "                # Draw red bounding box around the vehicle\n",
    "                (x, y, w, h) = stationary_objects[predicted_class]['bbox']\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "            else:\n",
    "                stationary_objects[predicted_class]['bbox'] = bbox\n",
    "    else:\n",
    "        # Reset the timer if the object moves\n",
    "        if predicted_class in stationary_objects:\n",
    "            del stationary_objects[predicted_class]\n",
    "\n",
    "    # Display the frame with bounding boxes\n",
    "    cv2.imshow('Static Object Detection', frame)\n",
    "\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57779571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 107ms/step\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 141ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "1/1 [==============================] - 0s 135ms/step\n",
      "1/1 [==============================] - 0s 133ms/step\n",
      "1/1 [==============================] - 0s 140ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "1/1 [==============================] - 0s 126ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 133ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 131ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 134ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 139ms/step\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 132ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "1/1 [==============================] - 0s 107ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model('vehicle_detection_model_resnet50_improved.h5')\n",
    "\n",
    "# Initialize video capture (0 for default camera, or provide video file path)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Parameters\n",
    "input_size = (128, 128)  # Input size for the model\n",
    "stationary_threshold = 15  # Time in seconds\n",
    "stationary_objects = {}  # Dictionary to track stationary objects\n",
    "\n",
    "def preprocess_frame(frame):\n",
    "    \"\"\"Preprocess frame for model prediction.\"\"\"\n",
    "    img = cv2.resize(frame, input_size)\n",
    "    img = img / 255.0  # Normalize\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    return img\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Get frame dimensions\n",
    "    height, width = frame.shape[:2]\n",
    "\n",
    "    # Preprocess the frame for prediction\n",
    "    img = preprocess_frame(frame)\n",
    "    \n",
    "    # Predict using the model\n",
    "    predictions = model.predict(img)\n",
    "    predicted_class = np.argmax(predictions, axis=1)[0]\n",
    "\n",
    "    # Only consider vehicle classes (assuming vehicle classes are labeled from 1 to N)\n",
    "    if predicted_class in [0, 1, 2, 3, 4, 5, 6, 7]:  # Adjust based on your class indices\n",
    "        # For simplicity, assume the entire frame is the bounding box\n",
    "        # In a real scenario, use an object detection model to get actual bounding boxes\n",
    "        bbox = (100, 100, 200, 200)  # Replace this with actual bounding box coordinates from detection model\n",
    "\n",
    "        # Track stationary objects\n",
    "        if predicted_class not in stationary_objects:\n",
    "            stationary_objects[predicted_class] = {'start_time': time.time(), 'bbox': bbox}\n",
    "        else:\n",
    "            elapsed_time = time.time() - stationary_objects[predicted_class]['start_time']\n",
    "            if elapsed_time > stationary_threshold:\n",
    "                # Draw red bounding box around the vehicle\n",
    "                (x, y, w, h) = stationary_objects[predicted_class]['bbox']\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "            else:\n",
    "                stationary_objects[predicted_class]['bbox'] = bbox\n",
    "    else:\n",
    "        # Reset the timer if the object moves\n",
    "        if predicted_class in stationary_objects:\n",
    "            del stationary_objects[predicted_class]\n",
    "\n",
    "    # Display the frame with bounding boxes\n",
    "    cv2.imshow('Static Object Detection', frame)\n",
    "\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b05b65f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model('vehicle_detection_model.h5')\n",
    "\n",
    "# Initialize video capture (0 for default camera, or provide video file path)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Parameters\n",
    "input_size = (128, 128)  # Input size for the model\n",
    "stationary_threshold = 15  # Time in seconds (2 minutes)\n",
    "stationary_objects = {}  # Dictionary to track stationary objects\n",
    "\n",
    "def preprocess_frame(frame):\n",
    "    \"\"\"Preprocess frame for model prediction.\"\"\"\n",
    "    img = cv2.resize(frame, input_size)\n",
    "    img = img / 255.0  # Normalize\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    return img\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Preprocess the frame for prediction\n",
    "    img = preprocess_frame(frame)\n",
    "    \n",
    "    # Predict using the model\n",
    "    predictions = model.predict(img)\n",
    "    predicted_class = np.argmax(predictions, axis=1)[0]\n",
    "\n",
    "    # Only consider vehicle classes (assuming vehicle classes are labeled from 1 to N)\n",
    "    if predicted_class in [0, 1, 2, 3, 4, 5, 6, 7]:  # Adjust based on your class indices\n",
    "        # For simplicity, assume the entire frame is the bounding box\n",
    "        bbox = (0, 0, frame.shape[1], frame.shape[0])  # You would normally get this from your detection model\n",
    "        \n",
    "        # Track stationary objects\n",
    "        if predicted_class not in stationary_objects:\n",
    "            stationary_objects[predicted_class] = {'start_time': time.time(), 'bbox': bbox}\n",
    "        else:\n",
    "            elapsed_time = time.time() - stationary_objects[predicted_class]['start_time']\n",
    "            if elapsed_time > stationary_threshold:\n",
    "                # Draw red bounding box around the vehicle\n",
    "                (x, y, w, h) = stationary_objects[predicted_class]['bbox']\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "            else:\n",
    "                stationary_objects[predicted_class]['bbox'] = bbox\n",
    "    else:\n",
    "        # Remove non-stationary or moved vehicles\n",
    "        stationary_objects = {k: v for k, v in stationary_objects.items() if time.time() - v['start_time'] <= stationary_threshold}\n",
    "\n",
    "    # Display the frame with bounding boxes\n",
    "    cv2.imshow('Static Object Detection', frame)\n",
    "\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33b3b6b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.9.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\dnn\\src\\darknet\\darknet_importer.cpp:210: error: (-212:Parsing error) Failed to open NetParameter file: yolov3.cfg in function 'cv::dnn::dnn4_v20231225::readNetFromDarknet'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Load YOLO\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m net \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mdnn\u001b[38;5;241m.\u001b[39mreadNet(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myolov3.weights\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myolov3.cfg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m layer_names \u001b[38;5;241m=\u001b[39m net\u001b[38;5;241m.\u001b[39mgetLayerNames()\n\u001b[0;32m      8\u001b[0m output_layers \u001b[38;5;241m=\u001b[39m [layer_names[i[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m net\u001b[38;5;241m.\u001b[39mgetUnconnectedOutLayers()]\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.9.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\dnn\\src\\darknet\\darknet_importer.cpp:210: error: (-212:Parsing error) Failed to open NetParameter file: yolov3.cfg in function 'cv::dnn::dnn4_v20231225::readNetFromDarknet'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Load YOLO\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Initialize video capture (0 for default camera, or provide video file path)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Parameters\n",
    "stationary_threshold = 120  # Time in seconds (2 minutes)\n",
    "stationary_objects = {}  # Dictionary to track stationary objects\n",
    "\n",
    "def preprocess_frame(frame):\n",
    "    \"\"\"Preprocess frame for model prediction.\"\"\"\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    return net.forward(output_layers)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Preprocess the frame for prediction\n",
    "    detections = preprocess_frame(frame)\n",
    "\n",
    "    # Get frame dimensions\n",
    "    height, width = frame.shape[:2]\n",
    "\n",
    "    # Process detections\n",
    "    for out in detections:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "\n",
    "            # Filter for vehicles with confidence threshold\n",
    "            if class_id in [2, 3, 5, 7]:  # COCO classes for car, motorcycle, bus, and truck\n",
    "                if confidence > 0.5:\n",
    "                    # Get bounding box coordinates\n",
    "                    center_x = int(detection[0] * width)\n",
    "                    center_y = int(detection[1] * height)\n",
    "                    w = int(detection[2] * width)\n",
    "                    h = int(detection[3] * height)\n",
    "                    x = int(center_x - w / 2)\n",
    "                    y = int(center_y - h / 2)\n",
    "\n",
    "                    bbox = (x, y, w, h)\n",
    "\n",
    "                    # Track stationary objects\n",
    "                    if class_id not in stationary_objects:\n",
    "                        stationary_objects[class_id] = {'start_time': time.time(), 'bbox': bbox}\n",
    "                    else:\n",
    "                        elapsed_time = time.time() - stationary_objects[class_id]['start_time']\n",
    "                        if elapsed_time > stationary_threshold:\n",
    "                            # Draw red bounding box around the vehicle\n",
    "                            (x, y, w, h) = stationary_objects[class_id]['bbox']\n",
    "                            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "                        else:\n",
    "                            stationary_objects[class_id]['bbox'] = bbox\n",
    "\n",
    "    # Remove non-stationary or moved vehicles\n",
    "    stationary_objects = {k: v for k, v in stationary_objects.items() if time.time() - v['start_time'] <= stationary_threshold}\n",
    "\n",
    "    # Display the frame with bounding boxes\n",
    "    cv2.imshow('Static Object Detection', frame)\n",
    "\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "224383bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\mahnoor/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2024-3-25 Python-3.11.4 torch-2.2.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Load YOLOv5 model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')  # You can use 'yolov5m', 'yolov5l', 'yolov5x' for larger models\n",
    "\n",
    "# Initialize video capture (0 for default camera, or provide video file path)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Parameters\n",
    "stationary_threshold = 10  # Time in seconds (2 minutes)\n",
    "stationary_objects = {}  # Dictionary to track stationary objects\n",
    "\n",
    "def detect_vehicles(frame):\n",
    "    \"\"\"Detect vehicles using YOLOv5 model.\"\"\"\n",
    "    results = model(frame)\n",
    "    return results.pandas().xyxy[0]  # Extract bounding box data as pandas dataframe\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Detect vehicles in the frame\n",
    "    detections = detect_vehicles(frame)\n",
    "\n",
    "    # Process detections\n",
    "    for index, row in detections.iterrows():\n",
    "        class_id = int(row['class'])\n",
    "        confidence = row['confidence']\n",
    "        if class_id in [2, 3, 5, 7] and confidence > 0.5:  # Classes: car, motorcycle, bus, truck\n",
    "            x, y, w, h = int(row['xmin']), int(row['ymin']), int(row['xmax'] - row['xmin']), int(row['ymax'] - row['ymin'])\n",
    "            bbox = (x, y, w, h)\n",
    "\n",
    "            # Track stationary objects\n",
    "            if class_id not in stationary_objects:\n",
    "                stationary_objects[class_id] = {'start_time': time.time(), 'bbox': bbox}\n",
    "            else:\n",
    "                elapsed_time = time.time() - stationary_objects[class_id]['start_time']\n",
    "                if elapsed_time > stationary_threshold:\n",
    "                    # Draw red bounding box around the vehicle\n",
    "                    (x, y, w, h) = stationary_objects[class_id]['bbox']\n",
    "                    cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "                else:\n",
    "                    stationary_objects[class_id]['bbox'] = bbox\n",
    "\n",
    "    # Remove non-stationary or moved vehicles\n",
    "    stationary_objects = {k: v for k, v in stationary_objects.items() if time.time() - v['start_time'] <= stationary_threshold}\n",
    "\n",
    "    # Display the frame with bounding boxes\n",
    "    cv2.imshow('Static Object Detection', frame)\n",
    "\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eb79385c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\mahnoor/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2024-3-25 Python-3.11.4 torch-2.2.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Load YOLOv5 model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')  # You can use 'yolov5m', 'yolov5l', 'yolov5x' for larger models\n",
    "\n",
    "# Initialize video capture (0 for default camera, or provide video file path)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Parameters\n",
    "stationary_threshold = 2  # Time in seconds (2 minutes)\n",
    "stationary_objects = {}  # Dictionary to track stationary objects\n",
    "\n",
    "def detect_vehicles(frame):\n",
    "    \"\"\"Detect vehicles using YOLOv5 model.\"\"\"\n",
    "    results = model(frame)\n",
    "    return results.pandas().xyxy[0]  # Extract bounding box data as pandas dataframe\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Detect vehicles in the frame\n",
    "    detections = detect_vehicles(frame)\n",
    "\n",
    "    # Get frame dimensions\n",
    "    height, width = frame.shape[:2]\n",
    "\n",
    "    # Process detections\n",
    "    for index, row in detections.iterrows():\n",
    "        class_id = int(row['class'])\n",
    "        confidence = row['confidence']\n",
    "        if class_id in [2, 3, 5, 7] and confidence > 0.5:  # Classes: car, motorcycle, bus, truck\n",
    "            x, y, w, h = int(row['xmin']), int(row['ymin']), int(row['xmax'] - row['xmin']), int(row['ymax'] - row['ymin'])\n",
    "            bbox = (x, y, w, h)\n",
    "\n",
    "            # Track stationary objects\n",
    "            if class_id not in stationary_objects:\n",
    "                stationary_objects[class_id] = {'start_time': time.time(), 'bbox': bbox, 'detected_time': time.time()}\n",
    "            else:\n",
    "                stationary_objects[class_id]['bbox'] = bbox\n",
    "                stationary_objects[class_id]['detected_time'] = time.time()\n",
    "\n",
    "    # Draw bounding boxes for all tracked objects\n",
    "    current_time = time.time()\n",
    "    for obj_id, obj_data in stationary_objects.items():\n",
    "        x, y, w, h = obj_data['bbox']\n",
    "        elapsed_time = current_time - obj_data['detected_time']\n",
    "        color = (0, 255, 0)  # Green by default\n",
    "        if elapsed_time > stationary_threshold:\n",
    "            color = (0, 0, 255)  # Red if stationary for too long\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "\n",
    "    # Remove non-stationary or moved vehicles\n",
    "    stationary_objects = {k: v for k, v in stationary_objects.items() if current_time - v['detected_time'] <= stationary_threshold}\n",
    "\n",
    "    # Display the frame with bounding boxes\n",
    "    cv2.imshow('Static Object Detection', frame)\n",
    "\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4faf7c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\mahnoor/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2024-3-25 Python-3.11.4 torch-2.2.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Load YOLOv5 model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')  # You can use 'yolov5m', 'yolov5l', 'yolov5x' for larger models\n",
    "\n",
    "# Initialize video capture (0 for default camera, or provide video file path)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Parameters\n",
    "stationary_threshold = 2  # Time in seconds (2 minutes)\n",
    "stationary_objects = {}  # Dictionary to track stationary objects\n",
    "\n",
    "def detect_vehicles(frame):\n",
    "    \"\"\"Detect vehicles using YOLOv5 model.\"\"\"\n",
    "    results = model(frame)\n",
    "    return results.pandas().xyxy[0]  # Extract bounding box data as pandas dataframe\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Detect vehicles in the frame\n",
    "    detections = detect_vehicles(frame)\n",
    "\n",
    "    # Get frame dimensions\n",
    "    height, width = frame.shape[:2]\n",
    "\n",
    "    # Process detections\n",
    "    for index, row in detections.iterrows():\n",
    "        class_id = int(row['class'])\n",
    "        confidence = row['confidence']\n",
    "        if class_id in [2, 3, 5, 7] and confidence > 0.5:  # Classes: car, motorcycle, bus, truck\n",
    "            x, y, w, h = int(row['xmin']), int(row['ymin']), int(row['xmax'] - row['xmin']), int(row['ymax'] - row['ymin'])\n",
    "            bbox = (x, y, w, h)\n",
    "\n",
    "            # Track stationary objects\n",
    "            if class_id not in stationary_objects:\n",
    "                stationary_objects[class_id] = {'start_time': time.time(), 'bbox': bbox, 'color': (0, 255, 0)}\n",
    "            else:\n",
    "                stationary_objects[class_id]['bbox'] = bbox\n",
    "                if stationary_objects[class_id]['color'] == (0, 255, 0):  # If the box is still green\n",
    "                    elapsed_time = time.time() - stationary_objects[class_id]['start_time']\n",
    "                    if elapsed_time > stationary_threshold:\n",
    "                        stationary_objects[class_id]['color'] = (0, 0, 255)  # Turn red if stationary too long\n",
    "\n",
    "    # Draw bounding boxes for all tracked objects\n",
    "    for obj_id, obj_data in stationary_objects.items():\n",
    "        x, y, w, h = obj_data['bbox']\n",
    "        color = obj_data['color']\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "\n",
    "    # Remove non-stationary or moved vehicles\n",
    "    stationary_objects = {k: v for k, v in stationary_objects.items() if current_time - v['start_time'] <= stationary_threshold or v['color'] == (0, 0, 255)}\n",
    "\n",
    "    # Display the frame with bounding boxes\n",
    "    cv2.imshow('Static Object Detection', frame)\n",
    "\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ee13a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\mahnoor/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2024-3-25 Python-3.11.4 torch-2.2.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Load YOLOv5 model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')  # You can use 'yolov5m', 'yolov5l', 'yolov5x' for larger models\n",
    "\n",
    "# Initialize video capture (0 for default camera, or provide video file path)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Parameters\n",
    "stationary_threshold = 2  # Time in seconds\n",
    "stationary_objects = {}  # Dictionary to track stationary objects\n",
    "\n",
    "def detect_vehicles(frame):\n",
    "    \"\"\"Detect vehicles using YOLOv5 model.\"\"\"\n",
    "    results = model(frame)\n",
    "    return results.pandas().xyxy[0]  # Extract bounding box data as pandas dataframe\n",
    "\n",
    "def get_unique_id(bbox, existing_ids):\n",
    "    \"\"\"Generate a unique ID for a detected object based on its bounding box.\"\"\"\n",
    "    x, y, w, h = bbox\n",
    "    unique_id = f\"{x}_{y}_{w}_{h}\"\n",
    "    while unique_id in existing_ids:\n",
    "        unique_id += \"_1\"\n",
    "    return unique_id\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Detect vehicles in the frame\n",
    "    detections = detect_vehicles(frame)\n",
    "\n",
    "    # Get frame dimensions\n",
    "    height, width = frame.shape[:2]\n",
    "\n",
    "    current_time = time.time()\n",
    "\n",
    "    # List of currently detected objects' IDs\n",
    "    current_ids = []\n",
    "\n",
    "    # Process detections\n",
    "    for index, row in detections.iterrows():\n",
    "        class_id = int(row['class'])\n",
    "        confidence = row['confidence']\n",
    "        if class_id in [2, 3, 5, 7] and confidence > 0.5:  # Classes: car, motorcycle, bus, truck\n",
    "            x, y, w, h = int(row['xmin']), int(row['ymin']), int(row['xmax'] - row['xmin']), int(row['ymax'] - row['ymin'])\n",
    "            bbox = (x, y, w, h)\n",
    "\n",
    "            unique_id = get_unique_id(bbox, stationary_objects.keys())\n",
    "            current_ids.append(unique_id)\n",
    "\n",
    "            # Track stationary objects\n",
    "            if unique_id not in stationary_objects:\n",
    "                stationary_objects[unique_id] = {'start_time': current_time, 'bbox': bbox, 'color': (0, 255, 0)}\n",
    "            else:\n",
    "                stationary_objects[unique_id]['bbox'] = bbox\n",
    "                elapsed_time = current_time - stationary_objects[unique_id]['start_time']\n",
    "                if elapsed_time > stationary_threshold:\n",
    "                    stationary_objects[unique_id]['color'] = (0, 0, 255)  # Turn red if stationary too long\n",
    "\n",
    "    # Remove objects not detected in the current frame\n",
    "    stationary_objects = {k: v for k, v in stationary_objects.items() if k in current_ids}\n",
    "\n",
    "    # Draw bounding boxes for all tracked objects\n",
    "    for obj_id, obj_data in stationary_objects.items():\n",
    "        x, y, w, h = obj_data['bbox']\n",
    "        color = obj_data['color']\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "\n",
    "    # Display the frame with bounding boxes\n",
    "    cv2.imshow('Static Object Detection', frame)\n",
    "\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
